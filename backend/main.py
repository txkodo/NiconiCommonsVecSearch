"""
NiconiCommonsVecSearch Backend API
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import logging
import time
from contextlib import asynccontextmanager

from vector_processor import get_vector_processor, VectorProcessor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
class VectorizeRequest(BaseModel):
    """ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«"""
    keyword: str = Field(..., min_length=1, max_length=1000, description="ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")


class BatchVectorizeRequest(BaseModel):
    """ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«"""
    keywords: List[str] = Field(..., min_items=1, max_items=100, description="ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ")


class VectorizeResponse(BaseModel):
    """ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«"""
    keyword: str = Field(..., description="å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    vector: List[float] = Field(..., description="ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾")
    dimension: int = Field(..., description="ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°")
    processing_time: float = Field(..., description="å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰")
    model_info: Dict[str, Any] = Field(..., description="ãƒ¢ãƒ‡ãƒ«æƒ…å ±")


class BatchVectorizeResponse(BaseModel):
    """ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«"""
    results: List[VectorizeResponse] = Field(..., description="ãƒ™ã‚¯ãƒˆãƒ«åŒ–çµæœã®ãƒªã‚¹ãƒˆ")
    total_count: int = Field(..., description="å‡¦ç†ã•ã‚ŒãŸç·æ•°")
    processing_time: float = Field(..., description="ç·å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰")


class HealthResponse(BaseModel):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«"""
    status: str = Field(..., description="ã‚µãƒ¼ãƒ“ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
    timestamp: float = Field(..., description="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—")
    model_status: Dict[str, Any] = Field(..., description="ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")


# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†"""
    # èµ·å‹•æ™‚å‡¦ç†
    logger.info("ğŸš€ Starting NiconiCommonsVecSearch Backend API...")
    
    # CLAPãƒ¢ãƒ‡ãƒ«ã®äº‹å‰åˆæœŸåŒ–ã§ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆè»½æ¸›
    try:
        logger.info("ğŸ”¥ Pre-warming CLAP model to reduce cold start latency...")
        start_time = time.time()
        
        vector_processor = get_vector_processor()
        # è»½é‡ãªãƒ†ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        test_result = vector_processor.vectorize_keyword("warmup test")
        
        warmup_time = time.time() - start_time
        logger.info(f"âœ… Model pre-warmed successfully in {warmup_time:.2f}s")
        logger.info(f"   Ready to serve requests with ~{test_result['dimension']}D vectors")
        
    except Exception as e:
        logger.warning(f"âš ï¸  Model pre-warming failed: {e}")
        logger.warning("   First API request will experience cold start delay")
    
    yield
    
    # çµ‚äº†æ™‚å‡¦ç†
    logger.info("ğŸ›‘ Shutting down NiconiCommonsVecSearch Backend API...")


# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
app = FastAPI(
    title="NiconiCommonsVecSearch API",
    description="ãƒ‹ã‚³ãƒ‹ãƒ»ã‚³ãƒ¢ãƒ³ã‚ºæ¥½æ›²æ¤œç´¢ã®ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«åŒ–API",
    version="1.0.0",
    lifespan=lifespan
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ãªã‚ªãƒªã‚¸ãƒ³ã‚’è¨­å®š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/", response_model=Dict[str, str])
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "NiconiCommonsVecSearch Backend API",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        vector_processor = get_vector_processor()
        model_info = vector_processor.get_model_info()
        
        return HealthResponse(
            status="healthy",
            timestamp=time.time(),
            model_status=model_info
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail="Service unhealthy")


@app.post("/api/vectorize", response_model=VectorizeResponse)
async def vectorize_keyword(request: VectorizeRequest):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    Args:
        request: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
    Returns:
        ãƒ™ã‚¯ãƒˆãƒ«åŒ–çµæœ
    """
    start_time = time.time()
    
    try:
        logger.info(f"Received vectorization request for keyword: {request.keyword}")
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†
        vector_processor = get_vector_processor()
        result = vector_processor.vectorize_keyword(request.keyword)
        
        processing_time = time.time() - start_time
        
        response = VectorizeResponse(
            keyword=result["keyword"],
            vector=result["vector"],
            dimension=result["dimension"],
            processing_time=processing_time,
            model_info=result["model_info"]
        )
        
        logger.info(f"Successfully vectorized keyword in {processing_time:.3f}s")
        return response
        
    except ValueError as e:
        logger.error(f"Invalid input: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Vectorization failed: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@app.post("/api/vectorize/batch", response_model=BatchVectorizeResponse)
async def vectorize_keywords_batch(request: BatchVectorizeRequest):
    """
    è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¸€æ‹¬ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    Args:
        request: ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
    Returns:
        ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–çµæœ
    """
    start_time = time.time()
    
    try:
        logger.info(f"Received batch vectorization request for {len(request.keywords)} keywords")
        
        # ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†
        vector_processor = get_vector_processor()
        results = vector_processor.vectorize_batch(request.keywords)
        
        processing_time = time.time() - start_time
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å¤‰æ›
        vectorize_responses = []
        for result in results:
            vectorize_responses.append(VectorizeResponse(
                keyword=result["keyword"],
                vector=result["vector"],
                dimension=result["dimension"],
                processing_time=processing_time / len(results),  # å¹³å‡å‡¦ç†æ™‚é–“
                model_info=result["model_info"]
            ))
        
        response = BatchVectorizeResponse(
            results=vectorize_responses,
            total_count=len(vectorize_responses),
            processing_time=processing_time
        )
        
        logger.info(f"Successfully vectorized {len(results)} keywords in {processing_time:.3f}s")
        return response
        
    except ValueError as e:
        logger.error(f"Invalid input: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Batch vectorization failed: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@app.get("/api/model/info", response_model=Dict[str, Any])
async def get_model_info():
    """
    ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    Returns:
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    """
    try:
        vector_processor = get_vector_processor()
        model_info = vector_processor.get_model_info()
        return model_info
    except Exception as e:
        logger.error(f"Failed to get model info: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)